{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with the Keras functional API\n",
    "\n",
    "The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
    "\n",
    "In my opinion this is the api that you want to always use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First example: a densely-connected network\n",
    "\n",
    "Some things to note:\n",
    "\n",
    "* A layer instance is callable (on a tensor), and it returns a tensor\n",
    "* Input tensor(s) and output tensor(s) can then be used to define a Model\n",
    "* Such a model can be trained just like Keras Sequential models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the only things we had to reference are the input and output tensors. We could make as complex interactions as we want, but all the model cares about are the inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All models are callable, just like layers\n",
    "\n",
    "With the functional API, it is easy to re-use trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just re-using the architecture of the model, you are also re-using its weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(784,))\n",
    "# This works, and returns the 10-way softmax we defined above.\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can allow, for instance, to quickly create models that can process sequences of inputs. You could turn an image classification model into a video classification model, in just one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# Input tensor for sequences of 20 timesteps,\n",
    "# each containing a 784-dimensional vector\n",
    "input_sequences = Input(shape=(20, 784))\n",
    "\n",
    "# This applies our previous model to every timestep in the input sequences.\n",
    "# the output of the previous model was a 10-way softmax,\n",
    "# so the output of the layer below will be a sequence of 20 vectors of size 10.\n",
    "processed_sequences = TimeDistributed(model)(input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-input and multi-output models\n",
    "\n",
    "Here's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.\n",
    "\n",
    "consider the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "\n",
    "x_in = Input(shape=(100,), name='x_in')\n",
    "y_in = Input(shape=(100,), name='y_in')\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64, activation='relu')(x_in)\n",
    "y = Dense(64, activation='relu')(y_in)\n",
    "\n",
    "z = concatenate([x, y])\n",
    "\n",
    "x = Dense(1, activation='sigmoid', name='x_out')(z)\n",
    "y = Dense(10, activation='softmax', name='y_out')(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a model with multiple inputs or outputs, you just need to specify a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[x_in, y_in], outputs=[x, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now a couple of ways to compile the model. First is just by passing in lists of losses and loss weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 0s - loss: 1.2015 - x_out_loss: 0.7267 - y_out_loss: 2.3741     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113cc8290>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "xs = np.random.randint(2, size=(1000, 1))\n",
    "ys = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=['binary_crossentropy', 'categorical_crossentropy'],\n",
    "              loss_weights=[1., 0.2])\n",
    "\n",
    "model.fit([data, data], [xs, to_categorical(ys)],\n",
    "          epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is to specify a dictionary (refering to the names of the output tensors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 0s - loss: 1.1621 - x_out_loss: 0.6944 - y_out_loss: 2.3383     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x114e48f50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'x_out': 'binary_crossentropy', 'y_out': 'categorical_crossentropy'},\n",
    "              loss_weights={'x_out': 1., 'y_out': 0.2})\n",
    "\n",
    "# And trained it via:\n",
    "model.fit({'x_in': data, 'y_in': data},\n",
    "          {'x_out': xs, 'y_out': to_categorical(ys)},\n",
    "          epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared layers\n",
    "\n",
    "Another good use for the functional API are models that use shared layers. Let's take a look at shared layers.\n",
    "\n",
    "The use is somewhat simple. We save the layer we want to use and apply it multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(64,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "layer_we_share = Dense(64, activation='relu')\n",
    "\n",
    "# Now we apply the layer twice\n",
    "x = layer_we_share(inputs)\n",
    "x = layer_we_share(x)\n",
    "\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The concept of layer \"node\"\n",
    "\n",
    "Whenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a \"node\" to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 0, 1, 2...\n",
    "\n",
    "In previous versions of Keras, you could obtain the output tensor of a layer instance via layer.get_output(), or its output shape via layer.output_shape. You still can (except get_output() has been replaced by the property output). But what if a layer is connected to multiple inputs?\n",
    "\n",
    "As long as a layer is only connected to one input, there is no confusion, and .output will return the one output of the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = Input(shape=(140, 256))\n",
    "\n",
    "dense = Dense(32)\n",
    "affine_a = dense(a)\n",
    "\n",
    "assert dense.output == affine_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so if the layer has multiple inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Layer dense_16 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-87b202ce46e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maffine_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/nate/Desktop/keras-tutorial/env/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             raise AttributeError('Layer ' + self.name +\n\u001b[0;32m--> 912\u001b[0;31m                                  \u001b[0;34m' has multiple inbound nodes, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m                                  \u001b[0;34m'hence the notion of \"layer output\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                                  \u001b[0;34m'is ill-defined. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Layer dense_16 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead."
     ]
    }
   ],
   "source": [
    "a = Input(shape=(140, 256))\n",
    "b = Input(shape=(140, 256))\n",
    "\n",
    "dense = Dense(32)\n",
    "affine_a = dense(a)\n",
    "affine_b = dense(b)\n",
    "\n",
    "dense.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay then. The following works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert dense.get_output_at(0) == affine_a\n",
    "assert dense.get_output_at(1) == affine_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple enough, right?\n",
    "\n",
    "The same is true for the properties input_shape and output_shape: as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of \"layer output/input shape\" is well defined, and that one shape will be returned by layer.output_shape/layer.input_shape. But if, for instance, you apply a same Conv2D layer to an input of shape (3, 32, 32), and then to an input of shape (3, 64, 64), the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
